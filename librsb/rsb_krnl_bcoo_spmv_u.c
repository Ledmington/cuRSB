/* @cond INNERDOC */
/*!
 @file
 @brief
 Performance kernels dispatching code, for each type, submatrix size, operation.
 For block coordinates format.
 Kernels unrolled, with no loops, for only user-specified blockings.
 */

/*

Copyright (C) 2008-2022 Michele Martone

This file is part of librsb.

librsb is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License as published
by the Free Software Foundation; either version 3 of the License, or
(at your option) any later version.

librsb is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
License for more details.

You should have received a copy of the GNU Lesser General Public
License along with librsb; see the file COPYING.
If not, see <http://www.gnu.org/licenses/>.

*/
/*
 The code in this file was generated automatically by an M4 script.
 It is not meant to be used as an API (Application Programming Interface).
 p.s.: right now, only row major matrix access is considered.

 */
/*!
 @file
 @brief
 Performance kernels dispatching code, for each type, submatrix size, operation.
 For block coordinates format.
 Kernels unrolled, with no loops, for only user-specified blockings.
 */

/*

Copyright (C) 2008-2022 Michele Martone

This file is part of librsb.

librsb is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License as published
by the Free Software Foundation; either version 3 of the License, or
(at your option) any later version.

librsb is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
License for more details.

You should have received a copy of the GNU Lesser General Public
License along with librsb; see the file COPYING.
If not, see <http://www.gnu.org/licenses/>.

*/
/*
 The code in this file was generated automatically by an M4 script.
 It is not meant to be used as an API (Application Programming Interface).
 p.s.: right now, only row major matrix access is considered.

 */
#include "rsb.h"
#include "rsb_common.h"
#include "rsb_internals.h"

#pragma GCC visibility push(hidden)

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_C__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_double_H__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_C__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_double_H__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_C__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_double_H__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // S
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + 1 * (roff - coff); // H
	double *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_C__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_double_H__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + incx * (roff - coff); // S
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + incx * (roff - coff); // S
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + incx * (roff - coff); // H
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + incx * (roff - coff); // H
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + incx * (roff - coff); // S
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + incx * (roff - coff); // S
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + incx * (roff - coff); // H
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double *trhs = rhs + incx * (roff - coff); // H
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_C__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_double_H__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tN_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tN_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tN_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tN_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tT_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tT_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tT_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tT_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tC_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tC_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tC_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tC_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tN_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tN_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tN_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tN_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tT_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tT_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tT_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tT_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tC_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_C__tC_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_C__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tC_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_H__tC_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *rhs, double *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_H__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tT_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tC_r1_c1_uu_sU_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + incx * (roff - coff); // S
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + incx * (roff - coff); // S
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tT_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tC_r1_c1_uu_sS_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + incx * (roff - coff); // H
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + incx * (roff - coff); // H
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tT_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tC_r1_c1_uu_sH_dE_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tT_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tC_r1_c1_uu_sU_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + incx * (roff - coff); // S
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + incx * (roff - coff); // S
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tT_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tC_r1_c1_uu_sS_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + incx * (roff - coff); // H
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	const double *trhs = rhs + incx * (roff - coff); // H
	double *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tT_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_C__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_H__tC_r1_c1_uu_sH_dI_uG(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_double_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tN_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tN_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tN_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tN_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tT_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tT_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tT_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tT_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tC_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tC_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tC_r1_c1_uu_sU_dE_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tC_r1_c1_uu_sU_dE_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		double aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tN_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tN_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tN_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tN_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tT_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tT_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tT_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tT_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tC_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_C__tC_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_C__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tC_r1_c1_uu_sU_dI_uU(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_H__tC_r1_c1_uu_sU_dI_uL(const double *restrict VA, const double *restrict rhs, double *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_H__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tN_r1_c1_uu_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tN_r1_c1_uu_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tT_r1_c1_uu_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tT_r1_c1_uu_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tC_r1_c1_uu_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tC_r1_c1_uu_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tN_r1_c1_uu_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tN_r1_c1_uu_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tT_r1_c1_uu_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tT_r1_c1_uu_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tC_r1_c1_uu_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tC_r1_c1_uu_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tN_r1_c1_uu_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tN_r1_c1_uu_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tT_r1_c1_uu_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tT_r1_c1_uu_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tC_r1_c1_uu_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tC_r1_c1_uu_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tN_r1_c1_uu_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tN_r1_c1_uu_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tT_r1_c1_uu_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tT_r1_c1_uu_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tC_r1_c1_uu_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tC_r1_c1_uu_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tN_r1_c1_uu_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tN_r1_c1_uu_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tT_r1_c1_uu_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tT_r1_c1_uu_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tC_r1_c1_uu_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tC_r1_c1_uu_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tN_r1_c1_uu_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tN_r1_c1_uu_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tT_r1_c1_uu_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tT_r1_c1_uu_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_C__tC_r1_c1_uu_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_H__tC_r1_c1_uu_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_C__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uaua_float_H__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uaua_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_C__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uauz_float_H__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uauz_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_C__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_uxua_float_H__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_uxua_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap);
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // S
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + 1 * (roff - coff); // H
	float *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_C__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_unua_float_H__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_unua_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + incx * (roff - coff); // S
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + incx * (roff - coff); // S
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + incx * (roff - coff); // H
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + incx * (roff - coff); // H
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + incx * (roff - coff); // S
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + incx * (roff - coff); // S
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + incx * (roff - coff); // H
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float *trhs = rhs + incx * (roff - coff); // H
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_C__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sasa_float_H__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sasa_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, incx, incy);
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tN_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tN_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tN_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tN_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tT_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tT_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tT_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tT_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tC_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tC_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tC_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tC_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tN_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tN_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tN_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tN_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tT_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tT_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tT_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tT_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tC_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_C__tC_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_C__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tC_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_H__tC_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *rhs, float *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_H__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tT_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tC_r1_c1_uu_sU_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + incx * (roff - coff); // S
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + incx * (roff - coff); // S
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tT_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tC_r1_c1_uu_sS_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sS_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + incx * (roff - coff); // H
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + incx * (roff - coff); // H
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tT_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tC_r1_c1_uu_sH_dE_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sH_dE_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tT_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tC_r1_c1_uu_sU_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + incx * (roff - coff); // S
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + incx * (roff - coff); // S
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tT_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tC_r1_c1_uu_sS_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sS_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + incx * (roff - coff); // H
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	const float *trhs = rhs + incx * (roff - coff); // H
	float *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tT_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_C__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_C__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_H__tC_r1_c1_uu_sH_dI_uG(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	/* Symmetric transposed reverts to symmetric not transposed */
	return rsb__BCOR_spmv_sxsa_float_H__tN_r1_c1_uu_sH_dI_uG(VA, rhs, out, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, alphap, incx, incy);
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tN_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tN_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tN_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tN_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tT_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tT_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tT_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tT_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tC_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tC_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tC_r1_c1_uu_sU_dE_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tC_r1_c1_uu_sU_dE_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		float aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tN_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tN_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tN_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tN_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tT_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tT_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tT_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tT_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tC_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_C__tC_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_C__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tC_r1_c1_uu_sU_dI_uU(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_H__tC_r1_c1_uu_sU_dI_uL(const float *restrict VA, const float *restrict rhs, float *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_H__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += fabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += fabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tN_r1_c1_uu_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tN_r1_c1_uu_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tT_r1_c1_uu_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tT_r1_c1_uu_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tC_r1_c1_uu_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tC_r1_c1_uu_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tN_r1_c1_uu_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tN_r1_c1_uu_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tT_r1_c1_uu_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tT_r1_c1_uu_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tC_r1_c1_uu_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tC_r1_c1_uu_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tN_r1_c1_uu_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tN_r1_c1_uu_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tT_r1_c1_uu_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tT_r1_c1_uu_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tC_r1_c1_uu_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tC_r1_c1_uu_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tN_r1_c1_uu_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tN_r1_c1_uu_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tT_r1_c1_uu_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tT_r1_c1_uu_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tC_r1_c1_uu_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tC_r1_c1_uu_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tN_r1_c1_uu_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tN_r1_c1_uu_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tT_r1_c1_uu_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tT_r1_c1_uu_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tC_r1_c1_uu_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tC_r1_c1_uu_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tN_r1_c1_uu_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tN_r1_c1_uu_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tT_r1_c1_uu_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tT_r1_c1_uu_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_C__tC_r1_c1_uu_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_H__tC_r1_c1_uu_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_float_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_FLOAT_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_float_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_float_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conjf(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * conjf(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * conjf(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * conjf(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conjf(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // S
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conjf(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conjf(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + 1 * (roff - coff); // H
	float complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conjf(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conjf(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conjf(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conjf(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conjf(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_float_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conjf(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += conjf(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += conjf(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += conjf(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conjf(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conjf(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += conjf(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += conjf(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += conjf(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conjf(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += conjf(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += conjf(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conjf(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += conjf(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += conjf(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += conjf(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conjf(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conjf(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += conjf(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += conjf(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += conjf(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conjf(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += conjf(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += conjf(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_float_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tN_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tN_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tN_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tN_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tT_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tT_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tT_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tT_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tC_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conjf(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tC_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conjf(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tC_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conjf(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tC_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conjf(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tN_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tN_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tN_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tN_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tT_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tT_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tT_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tT_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tC_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conjf(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_C__tC_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conjf(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_C__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tC_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conjf(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_float_complex_H__tC_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *rhs, float complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conjf(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_float_complex_H__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conjf(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*conjf(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*conjf(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*conjf(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conjf(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sU_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conjf(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*conjf(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*conjf(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*conjf(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conjf(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sS_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sH_dE_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conjf(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*conjf(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*conjf(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*conjf(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conjf(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sU_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conjf(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*conjf(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*conjf(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*conjf(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conjf(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sS_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // S
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conjf(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conjf(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sH_dI_uG(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	const float complex *trhs = rhs + incx * (roff - coff); // H
	float complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conjf(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conjf(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conjf(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conjf(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conjf(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_float_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tN_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tN_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tN_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tN_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tT_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tT_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tT_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tT_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tC_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conjf(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tC_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conjf(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tC_r1_c1_uu_sU_dE_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conjf(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tC_r1_c1_uu_sU_dE_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		float complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((float complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conjf(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tN_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tN_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tN_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tN_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tT_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tT_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tT_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tT_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tC_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conjf(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_C__tC_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conjf(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_C__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tC_r1_c1_uu_sU_dI_uU(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		float complex ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conjf(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_float_complex_H__tC_r1_c1_uu_sU_dI_uL(const float complex *restrict VA, const float complex *restrict rhs, float complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const float complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		float complex ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conjf(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_float_complex_H__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabsf(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabsf(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type float complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uaua_double_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, Mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += VA[n + 1] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += VA[n + 2] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += VA[n + 3] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += VA[n + 0] * trhs[i * 1];
			out[i * 1] += conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	rsb__cblas_Xscal(RSB_NUMERICAL_TYPE_DOUBLE_COMPLEX, mdim, NULL, out, 1);
	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uauz_double_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (alpha)*conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (alpha)*conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (alpha)*conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (alpha)*conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*VA[n + 1] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*VA[n + 2] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*VA[n + 3] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*VA[n + 0] * trhs[i * 1];
			out[i * 1] += (alpha)*conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (alpha)*conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (alpha)*VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (alpha)*conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (alpha)*conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (alpha)*conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (alpha)*conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (alpha)*VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_uxua_double_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * VA[n + 1] * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * VA[n + 2] * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * VA[n + 3] * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * VA[n + 0] * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (1)) + roff * (1);
	out = (out - roff * (1)) + coff * (1);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conj(VA[n + 0]) * rhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * 1] += (-1) * conj(VA[n + 1]) * rhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * 1] += (-1) * conj(VA[n + 2]) * rhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * 1] += (-1) * conj(VA[n + 3]) * rhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * 1] += (-1) * conj(VA[n + 0]) * rhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * VA[n] * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // S
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * 1] += (-1) * VA[n] * rhs[j * 1];
			if (RSB_LIKELY(i != j))
				out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * VA[n] * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * conj(VA[n]) * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * VA[n + 1] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 1]) * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * VA[n + 2] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 2]) * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * VA[n + 3] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 3]) * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * VA[n + 0] * trhs[i * 1];
			out[i * 1] += (-1) * conj(VA[n + 0]) * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y - {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + 1 * (roff - coff); // H
	double complex *tout = out + 1 * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * 1] += (-1) * conj(VA[n]) * rhs[i * 1];
			if (RSB_LIKELY(j != i))
				out[i * 1] += (-1) * VA[n] * rhs[j * 1];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * 1] += (-1) * conj(VA[n + 1]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 1] * rhs[j * 1];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * 1] += (-1) * conj(VA[n + 2]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 2] * rhs[j * 1];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * 1] += (-1) * conj(VA[n + 3]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 3] * rhs[j * 1];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * 1] += (-1) * conj(VA[n + 0]) * trhs[i * 1];
			out[i * 1] += (-1) * VA[n + 0] * rhs[j * 1];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_unua_double_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conj(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += conj(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += conj(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += conj(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conj(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conj(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += conj(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += conj(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += conj(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conj(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += conj(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += conj(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conj(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += conj(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += conj(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += conj(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conj(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conj(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += conj(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += conj(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += conj(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += conj(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += conj(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += conj(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^T} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += VA[n + 1] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += VA[n + 2] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += VA[n + 3] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += VA[n + 0] * trhs[i * incx];
			out[i * incy] += conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow y + {A^H} \cdot x, where A == A^H. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;

	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sasa_double_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tN_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tN_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tN_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tN_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		if (n == nnz || VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * 1] = (out[ii * 1] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tT_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tT_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tT_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tT_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tC_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conj(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tC_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conj(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tC_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * 1] /= aa;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conj(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tC_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * 1] /= aa;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conj(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tN_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tN_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tN_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tN_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * 1];
		}

		out[ii * 1] = (out[ii * 1] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tT_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tT_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tT_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tT_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^T}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= VA[n] * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tC_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conj(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_C__tC_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conj(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_C__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tC_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = out[ii * 1];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conj(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_uxua_double_complex_H__tC_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *rhs, double complex *out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow {A^H}^{-1} \cdot x, where A \neq A^T. \f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = out[ii * 1];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * 1] -= conj(VA[n]) * ax;
		}

		out[ii * 1] = (out[ii * 1]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_uxua_double_complex_H__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conj(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*conj(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*conj(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*conj(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conj(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sU_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conj(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*conj(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*conj(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*conj(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conj(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sU_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sS_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sS_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sH_dE_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sH_dE_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*VA[n + 1] * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*VA[n + 2] * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*VA[n + 3] * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*VA[n + 0] * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conj(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*conj(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*conj(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*conj(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conj(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sU_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A \neq A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rhs = (rhs - coff * (incx)) + roff * (incx);
	out = (out - roff * (incy)) + coff * (incy);
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conj(VA[n + 0]) * rhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[j * incy] += (alpha)*conj(VA[n + 1]) * rhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[j * incy] += (alpha)*conj(VA[n + 2]) * rhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[j * incy] += (alpha)*conj(VA[n + 3]) * rhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[j * incy] += (alpha)*conj(VA[n + 0]) * rhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sU_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sS_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^T. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // S
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sS_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
			if (RSB_LIKELY(i != j))
				out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tN_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^T} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*VA[n] * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*conj(VA[n]) * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*VA[n + 1] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 1]) * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*VA[n + 2] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 2]) * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*VA[n + 3] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 3]) * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*VA[n + 0] * trhs[i * incx];
			out[i * incy] += (alpha)*conj(VA[n + 0]) * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tT_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_C__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sH_dI_uG(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$y \leftarrow \beta \cdot y + \alpha \cdot {A^H} \cdot x, where A == A^H. \f$
	 * with incx and incy as x and y vector strides
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	/* NOTE: Diagonal implicit is not really handled here: look at caller level. */
	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	const double complex *trhs = rhs + incx * (roff - coff); // H
	double complex *tout = out + incy * (coff - roff);

	if (roff == coff)
		for (n = 0; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			out[j * incy] += (alpha)*conj(VA[n]) * rhs[i * incx];
			if (RSB_LIKELY(j != i))
				out[i * incy] += (alpha)*VA[n] * rhs[j * incx];
		}
	if (roff != coff)
	{
		for (n = 0; n + 3 < nnz; n += 4)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
			i = IA[n + 1];
			j = JA[n + 1];
			tout[j * incy] += (alpha)*conj(VA[n + 1]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 1] * rhs[j * incx];
			i = IA[n + 2];
			j = JA[n + 2];
			tout[j * incy] += (alpha)*conj(VA[n + 2]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 2] * rhs[j * incx];
			i = IA[n + 3];
			j = JA[n + 3];
			tout[j * incy] += (alpha)*conj(VA[n + 3]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 3] * rhs[j * incx];
		}
		for (; n < nnz; ++n)
		{
			i = IA[n + 0];
			j = JA[n + 0];
			tout[j * incy] += (alpha)*conj(VA[n + 0]) * trhs[i * incx];
			out[i * incy] += (alpha)*VA[n + 0] * rhs[j * incx];
		}
	}

	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spmv_sxsa_double_complex_H__tC_r1_c1_uu_sH_dI_uG\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tN_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tN_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tN_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		--n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tN_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tN_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		if (n == nnz || VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		out[ii * incy] = ((alpha)*out[ii * incy] - ax) / VA[n];
		++n;
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tN_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tT_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tT_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tT_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tT_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tT_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tT_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tC_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conj(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tC_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conj(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tC_r1_c1_uu_sU_dE_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		n++;
		out[ii * incy] /= aa;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conj(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tC_r1_c1_uu_sU_dE_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tC_r1_c1_uu_sU_dE_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		double complex aa;
		if (n >= nnz)
			return RSB_ERR_INVALID_NUMERICAL_DATA;
		aa = VA[n];
		if (VA[n] == ((double complex)(0)))
			return RSB_ERR_INVALID_NUMERICAL_DATA;

		n--;
		out[ii * incy] /= aa;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conj(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tC_r1_c1_uu_sU_dE_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tN_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tN_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tN_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = 0;

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tN_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tN_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = 0;
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii && j != i)))
				break;
			ax += VA[n] * out[j * incy];
		}

		out[ii * incy] = ((alpha)*out[ii * incy] - ax);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tN_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tT_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tT_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tT_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tT_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tT_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= VA[n] * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tT_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tC_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conj(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_C__tC_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conj(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_C__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tC_r1_c1_uu_sU_dI_uU(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;
	for (n = 0, ii = 0; RSB_LIKELY(ii < Mdim); ++ii)
	{
		double complex ax;
		ax = out[ii * incy];
		for (; RSB_LIKELY(n < nnz); ++n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conj(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tC_r1_c1_uu_sU_dI_uU\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_spsv_sxsx_double_complex_H__tC_r1_c1_uu_sU_dI_uL(const double complex *restrict VA, const double complex *restrict rhs, double complex *restrict out, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *restrict alphap, rsb_coo_idx_t incx, rsb_coo_idx_t incy)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	const double complex alpha = *alphap;
	rsb_coo_idx_t ii;

	for (n = nnz - 1, ii = Mdim - 1; RSB_LIKELY(ii + 1 > 0); --ii)
	{
		double complex ax;
		ax = out[ii * incy];

		for (; RSB_LIKELY(n + 1 > 0); --n)
		{
			i = IA[n];
			j = JA[n];
			if (RSB_UNLIKELY(!(i == ii)))
				break;
			out[j * incy] -= conj(VA[n]) * ax;
		}

		out[ii * incy] = ((alpha)*out[ii * incy]);
	}
	if (rsb__getenv_int_t("RSB_VERBOSE_KERNELS", 0))
		RSB_STDOUT("in rsb__BCOR_spsv_sxsx_double_complex_H__tC_r1_c1_uu_sU_dI_uL\n");
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{\infty} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{j=0}^{mdim} A_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += cabs(VA[n]);
		if (roff + i != coff + j)
			row_sums[coff + j] += cabs(VA[n]);
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		row_sums[roff + i] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_coo_idx_t i = 0, j = 0;
	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr, *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$ \|A\|_{1} \f$ (or rather, \f$ row\_sums_i \leftarrow \sum_{i=0}^{Mdim} A^{T}_{ij} ), where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	register rsb_half_idx_t i = 0, j = 0;
	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr, *JA = (const rsb_half_idx_t *)bindx;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		j = JA[n];
		row_sums[roff + i] += VA[n];
		if (roff + i != coff + j)
			row_sums[coff + j] += VA[n];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal explicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A \neq A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^T.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *IA = (const rsb_coo_idx_t *)bpntr;
	register rsb_coo_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *IA = (const rsb_half_idx_t *)bpntr;
	register rsb_half_idx_t i = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		i = IA[n];
		VA[n] *= scale_factors[i];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_coo_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_coo_idx_t *JA = (const rsb_coo_idx_t *)bindx;
	register rsb_coo_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/**
	 * \ingroup rsb_doc_kernels
	 * Computes \f$A \leftarrow A\cdot P, P_{ii}=s_{i}, where A == A^H.\f$
	 * A blocked 1 x 1, stored in BCOR format, diagonal implicit, of type double complex, with rsb_half_idx_t column indices.
	 * \return \rsb_errval_inp_param_msg
	 */

	const rsb_half_idx_t *JA = (const rsb_half_idx_t *)bindx;
	register rsb_half_idx_t j = 0;
	register rsb_nnz_idx_t n = 0;
	for (n = 0; RSB_LIKELY(n < nnz); ++n)
	{
		j = JA[n];
		VA[n] *= scale_factors[j];
	}
	return RSB_ERR_NO_ERROR;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tN_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tN_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tN_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tN_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tT_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tT_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tT_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tT_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tC_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tC_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tC_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tC_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tN_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tN_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tN_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tN_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tT_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tT_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tT_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tT_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tC_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tC_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tC_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tC_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tN_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tN_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tN_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tN_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tT_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tT_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tT_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tT_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tC_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_C_u_tC_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_C__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tC_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_H_u_tC_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_H__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tN_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tN_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tN_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tN_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tT_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tT_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tT_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tT_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tC_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tC_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tC_sU_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tC_sU_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tN_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tN_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tN_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tN_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tT_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tT_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tT_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tT_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tC_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tC_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tC_sS_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tC_sS_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tN_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tN_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tN_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tN_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tT_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tT_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tT_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tT_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tC_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_C_u_tC_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_C__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tC_sH_dE_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_H_u_tC_sH_dI_uG(const double *VA, double *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_H__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tN_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tN_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tN_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tN_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tN_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tN_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tN_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tN_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tT_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tT_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tT_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tT_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tT_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tT_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tT_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tT_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tC_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tC_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tC_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tC_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tC_sU_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tC_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tC_sU_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tC_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tN_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tN_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tN_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tN_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tN_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tN_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tN_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tN_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tT_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tT_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tT_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tT_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tT_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tT_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tT_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tT_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tC_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tC_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tC_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tC_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tC_sS_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tC_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tC_sS_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tC_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tN_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tN_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tN_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tN_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tN_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tN_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tN_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tN_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tT_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tT_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tT_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tT_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tT_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tT_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tT_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tT_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tC_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tC_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_C_u_tC_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_C__tC_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tC_sH_dE_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tC_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_H_u_tC_sH_dI_uG(double *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_H__tC_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tN_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tN_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tN_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tN_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tT_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tT_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tT_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tT_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tC_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tC_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tC_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tC_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tN_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tN_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tN_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tN_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tT_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tT_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tT_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tT_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tC_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tC_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tC_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tC_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tN_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tN_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tN_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tN_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tT_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tT_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tT_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tT_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tC_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_C_u_tC_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_C__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tC_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_H_u_tC_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_H__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tN_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tN_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tN_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tN_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tT_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tT_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tT_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tT_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tC_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tC_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tC_sU_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tC_sU_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tN_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tN_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tN_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tN_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tT_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tT_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tT_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tT_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tC_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tC_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tC_sS_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tC_sS_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tN_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tN_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tN_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tN_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tT_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tT_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tT_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tT_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tC_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_C_u_tC_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_C__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tC_sH_dE_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_H_u_tC_sH_dI_uG(const float *VA, float *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_H__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tN_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tN_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tN_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tN_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tN_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tN_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tN_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tN_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tT_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tT_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tT_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tT_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tT_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tT_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tT_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tT_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tC_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tC_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tC_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tC_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tC_sU_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tC_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tC_sU_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tC_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tN_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tN_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tN_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tN_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tN_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tN_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tN_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tN_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tT_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tT_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tT_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tT_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tT_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tT_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tT_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tT_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tC_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tC_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tC_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tC_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tC_sS_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tC_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tC_sS_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tC_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tN_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tN_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tN_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tN_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tN_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tN_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tN_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tN_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tT_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tT_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tT_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tT_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tT_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tT_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tT_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tT_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tC_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tC_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_C_u_tC_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_C__tC_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tC_sH_dE_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tC_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_H_u_tC_sH_dI_uG(float *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_H__tC_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tN_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tN_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tN_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tN_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tT_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tT_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tT_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tT_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tC_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tC_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tC_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tC_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tN_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tN_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tN_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tN_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tT_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tT_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tT_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tT_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tC_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tC_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tC_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tC_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tN_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tN_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tN_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tN_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tT_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tT_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tT_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tT_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tC_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_C_u_tC_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_C__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tC_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_float_complex_H_u_tC_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_float_complex_H__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tN_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tN_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tN_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tN_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tT_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tT_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tT_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tT_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tC_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tC_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tC_sU_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tC_sU_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tN_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tN_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tN_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tN_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tT_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tT_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tT_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tT_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tC_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tC_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tC_sS_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tC_sS_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tN_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tN_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tN_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tN_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tT_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tT_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tT_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tT_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tC_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_C_u_tC_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_C__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tC_sH_dE_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_float_complex_H_u_tC_sH_dI_uG(const float complex *VA, float complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_float_complex_H__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tN_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tN_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tN_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tN_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tT_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tT_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tT_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tT_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tC_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tC_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tC_sU_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tC_sU_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tN_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tN_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tN_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tN_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tT_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tT_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tT_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tT_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tC_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tC_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tC_sS_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tC_sS_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tN_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tN_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tN_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tN_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tN_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tN_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tT_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tT_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tT_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tT_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tT_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tT_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tC_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_C_u_tC_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_C__tC_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tC_sH_dE_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_float_complex_H_u_tC_sH_dI_uG(float complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const float complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_float_complex_H__tC_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tN_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tN_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tN_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tN_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tT_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tT_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tT_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tT_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tC_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tC_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tC_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tC_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tN_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tN_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tN_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tN_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tT_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tT_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tT_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tT_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tC_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tC_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tC_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tC_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tN_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tN_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tN_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tN_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tT_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tT_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tT_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tT_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tC_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_C_u_tC_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_C__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tC_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_infty_norm_double_complex_H_u_tC_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "infty_norm".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_infty_norm_double_complex_H__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tN_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tN_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tN_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tN_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tT_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tT_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tT_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tT_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tC_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tC_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tC_sU_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sU_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tC_sU_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sU_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tN_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tN_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tN_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tN_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tT_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tT_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tT_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tT_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tC_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tC_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tC_sS_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sS_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tC_sS_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sS_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tN_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tN_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tN_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tN_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tN_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tT_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tT_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tT_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tT_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tT_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tC_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_C_u_tC_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_C__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tC_sH_dE_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sH_dE_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_rowssums_double_complex_H_u_tC_sH_dI_uG(const double complex *VA, double complex *row_sums, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz)
{
	/*
	 * Select kernel function for operation "rowssums".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_rowssums_double_complex_H__tC_r1_c1_uu_sH_dI_uG(VA, row_sums, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tN_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tN_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tN_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tN_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tT_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tT_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tT_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tT_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tC_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tC_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tC_sU_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sU_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tC_sU_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sU_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tN_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tN_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tN_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tN_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tT_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tT_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tT_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tT_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tC_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tC_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tC_sS_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sS_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tC_sS_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sS_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tN_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tN_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tN_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tN_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tN_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tN_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tT_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tT_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tT_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tT_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tT_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tT_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tC_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_C_u_tC_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_coo_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_C__tC_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tC_sH_dE_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sH_dE_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

rsb_err_t rsb__BCOR_scale_double_complex_H_u_tC_sH_dI_uG(double complex *VA, const rsb_coo_idx_t Mdim, const rsb_coo_idx_t mdim, const rsb_half_idx_t *restrict bindx, const rsb_nnz_idx_t *restrict bpntr, const rsb_nnz_idx_t *restrict indptr, const rsb_coo_idx_t *restrict rpntr, const rsb_coo_idx_t *restrict cpntr, const rsb_coo_idx_t br, const rsb_coo_idx_t bc, const rsb_coo_idx_t roff, const rsb_coo_idx_t coff, const rsb_flags_t flags, const rsb_nnz_idx_t nnz, const double complex *scale_factors)
{
	/*
	 * Select kernel function for operation "scale".
	 *
	 * \return \rsb_errval_inp_param_msg
	 */
	rsb_err_t errval = RSB_ERR_NO_ERROR;

	errval = rsb__BCOR_scale_double_complex_H__tC_r1_c1_uu_sH_dI_uG(VA, Mdim, mdim, bindx, bpntr, indptr, rpntr, cpntr, br, bc, roff, coff, flags, nnz, scale_factors);
	return errval;
}

/* @endcond */
